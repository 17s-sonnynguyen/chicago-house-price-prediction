{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, leaves_list\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn. ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Chicago-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "df['sold_price'] = pd.to_numeric(df['sold_price'], errors='coerce')\n",
    "df_numeric = df.select_dtypes(include='number')\n",
    "\n",
    "correlation = (\n",
    "    df_numeric\n",
    "    .corr()['sold_price']\n",
    "    .dropna()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "correlation = correlation[correlation.index != 'sold_price']\n",
    "correlation_df = correlation.to_frame(name='Correlation').round(3)\n",
    "\n",
    "fig_heatmap = px.imshow(\n",
    "    correlation_df,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    "    title=\"Correlation Heatmap: Features vs Sold Price\"\n",
    ")\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "    autosize=True,\n",
    "    height=600,\n",
    "    margin=dict(l=60, r=60, t=80, b=60),\n",
    "    font=dict(size=14)\n",
    ")\n",
    "\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig_bar = px.bar(\n",
    "    correlation_df,\n",
    "    x='Correlation',\n",
    "    y=correlation_df.index,\n",
    "    orientation='h',\n",
    "    title=\"Feature Correlation Strength with Sold Price\",\n",
    "    text='Correlation'\n",
    ")\n",
    "\n",
    "fig_bar.update_layout(\n",
    "    autosize=True,\n",
    "    height=700,\n",
    "    margin=dict(l=120, r=60, t=80, b=60),\n",
    "    font=dict(size=14)\n",
    ")\n",
    "\n",
    "fig_bar.update_traces(textposition='outside')\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "top_feature = correlation_df.index[0]\n",
    "\n",
    "fig_scatter = px.scatter(\n",
    "    df,\n",
    "    x=top_feature,\n",
    "    y='sold_price',\n",
    "    trendline='ols',\n",
    "    title=f\"Sold Price vs {top_feature}\",\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig_scatter.update_layout(\n",
    "    autosize=True,\n",
    "    height=600,\n",
    "    margin=dict(l=60, r=60, t=80, b=60),\n",
    "    font=dict(size=14)\n",
    ")\n",
    "\n",
    "fig_scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Column Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop = ['id', 'property_id', 'listing_number', 'status_changed_on', 'created_at', 'updated_at', 'status', 'contracted_on', 'off_market_on', 'original_list_price', 'previous_price', 'seller_concessions', 'finished_square_feet', 'derived_basement_square_feet', 'car_storage', 'car_spaces', 'area', 'subdivision', 'street', 'state', 'county', 'property_key', 'externally_last_updated_at', 'photos',  'photos_pulled', 'structural_style', 'architecture', 'lot_size_square_feet', 'lot_size_acres', 'basement_finished_status', 'basement_finished_pct', 'basement_square_feet', 'basement_size', 'basement_type', 'seller_type', 'public_remarks', 'description', 'lat', 'lng', 'zoned', 'showings_phone', 'approval_condition', 'listing_agent', 'listing_brokerage', 'is_attached', 'stories', 'version', 'parcel_number', 'hoa_name', 'school_district', 'private_remarks', 'price_changes', 'unit_count', 'county_data_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop = df_columns_drop.drop(columns = columns_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_columns_drop.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert zip to number\n",
    "df_columns_drop['zip'] = pd.to_numeric(df_columns_drop['zip'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average days on market\n",
    "temp_df = df_columns_drop[['listed_on', 'sold_on']].copy()\n",
    "\n",
    "temp_df['listed_on'] = pd.to_datetime(temp_df['listed_on'], errors='coerce')\n",
    "temp_df['sold_on'] = pd.to_datetime(temp_df['sold_on'], errors='coerce')\n",
    "\n",
    "temp_df = temp_df.dropna(subset=['listed_on', 'sold_on'])\n",
    "\n",
    "temp_df['days_on_market'] = (temp_df['sold_on'] - temp_df['listed_on']).dt.days\n",
    "\n",
    "temp_df = temp_df[temp_df['days_on_market'] >= 0]\n",
    "\n",
    "average_days_on_market = temp_df['days_on_market'].mean()\n",
    "average_days_on_market"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the missing value by ascending order\n",
    "df_columns_drop.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if a row misses more than 10 values\n",
    "drop_threshold = 10\n",
    "df_columns_drop = df_columns_drop[df_columns_drop.isnull().sum(axis=1) <= drop_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start cleaning by this order\n",
    "df_columns_drop.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the zip by finding the mode of the same city\n",
    "zip_mode_by_city = (\n",
    "    df_columns_drop\n",
    "    .dropna(subset=['zip', 'city'])\n",
    "    .groupby('city')['zip']\n",
    "    .agg(lambda x: x.mode().iloc[0])\n",
    ")\n",
    "\n",
    "df_columns_drop['zip'] = df_columns_drop['zip'].fillna(\n",
    "    df_columns_drop['city'].map(zip_mode_by_city)\n",
    ")\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['zip']).drop(columns=['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean listed on by subtracting the sold_on with average market days\n",
    "df_columns_drop['sold_on'] = pd.to_datetime(df_columns_drop['sold_on'], errors='coerce')\n",
    "df_columns_drop['listed_on'] = pd.to_datetime(df_columns_drop['listed_on'], errors='coerce')\n",
    "\n",
    "missing_listed_mask = df_columns_drop['listed_on'].isna() & df_columns_drop['sold_on'].notna()\n",
    "df_columns_drop.loc[missing_listed_mask, 'listed_on'] = (\n",
    "    df_columns_drop.loc[missing_listed_mask, 'sold_on'] - pd.to_timedelta(average_days_on_market, unit='d')\n",
    ")\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['listed_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure numeric values\n",
    "df_columns_drop['baths'] = pd.to_numeric(df_columns_drop['baths'], errors='coerce')\n",
    "df_columns_drop['above_grade_square_feet'] = pd.to_numeric(df_columns_drop['above_grade_square_feet'], errors='coerce')\n",
    "\n",
    "# Fill by median of baths from similar houses (same range of square feet)\n",
    "size_tolerance = 200\n",
    "missing_baths_mask = df_columns_drop['baths'].isna() & df_columns_drop['above_grade_square_feet'].notna()\n",
    "\n",
    "for idx in df_columns_drop[missing_baths_mask].index:\n",
    "    target_sqft = df_columns_drop.loc[idx, 'above_grade_square_feet']\n",
    "    similar_houses = df_columns_drop[\n",
    "        (df_columns_drop['above_grade_square_feet'] >= target_sqft - size_tolerance) &\n",
    "        (df_columns_drop['above_grade_square_feet'] <= target_sqft + size_tolerance) &\n",
    "        (df_columns_drop['baths'].notna())\n",
    "    ]\n",
    "    if not similar_houses.empty:\n",
    "        df_columns_drop.loc[idx, 'baths'] = similar_houses['baths'].median()\n",
    "\n",
    "# Fill by the median from the same zip\n",
    "missing_baths_mask = df_columns_drop['baths'].isna() & df_columns_drop['zip'].notna()\n",
    "baths_median_by_zip = df_columns_drop.groupby('zip')['baths'].median()\n",
    "df_columns_drop.loc[missing_baths_mask, 'baths'] = df_columns_drop.loc[missing_baths_mask, 'zip'].map(baths_median_by_zip)\n",
    "\n",
    "# Drop if none method works\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['baths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same logic for beds\n",
    "df_columns_drop['beds'] = pd.to_numeric(df_columns_drop['beds'], errors='coerce')\n",
    "\n",
    "size_tolerance = 200\n",
    "missing_beds_mask = df_columns_drop['beds'].isna() & df_columns_drop['above_grade_square_feet'].notna()\n",
    "\n",
    "for idx in df_columns_drop[missing_beds_mask].index:\n",
    "    target_sqft = df_columns_drop.loc[idx, 'above_grade_square_feet']\n",
    "    similar_houses = df_columns_drop[\n",
    "        (df_columns_drop['above_grade_square_feet'] >= target_sqft - size_tolerance) &\n",
    "        (df_columns_drop['above_grade_square_feet'] <= target_sqft + size_tolerance) &\n",
    "        (df_columns_drop['beds'].notna())\n",
    "    ]\n",
    "    if not similar_houses.empty:\n",
    "        df_columns_drop.loc[idx, 'beds'] = similar_houses['beds'].median()\n",
    "\n",
    "missing_beds_mask = df_columns_drop['beds'].isna() & df_columns_drop['zip'].notna()\n",
    "beds_median_by_zip = df_columns_drop.groupby('zip')['beds'].median()\n",
    "df_columns_drop.loc[missing_beds_mask, 'beds'] = df_columns_drop.loc[missing_beds_mask, 'zip'].map(beds_median_by_zip)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['beds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean year_built by median from the same zip\n",
    "df_columns_drop['year_built'] = pd.to_numeric(df_columns_drop['year_built'], errors='coerce')\n",
    "\n",
    "year_built_median_by_zip = df_columns_drop.groupby('zip')['year_built'].median()\n",
    "\n",
    "missing_year_mask = df_columns_drop['year_built'].isna() & df_columns_drop['zip'].notna()\n",
    "df_columns_drop.loc[missing_year_mask, 'year_built'] = df_columns_drop.loc[missing_year_mask, 'zip'].map(year_built_median_by_zip)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['year_built'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same logic apply to clean total_square_feet\n",
    "df_columns_drop['total_square_feet'] = pd.to_numeric(df_columns_drop['total_square_feet'], errors='coerce')\n",
    "\n",
    "total_sqft_median_by_zip = df_columns_drop.groupby('zip')['total_square_feet'].median()\n",
    "\n",
    "missing_sqft_mask = df_columns_drop['total_square_feet'].isna() & df_columns_drop['zip'].notna()\n",
    "df_columns_drop.loc[missing_sqft_mask, 'total_square_feet'] = df_columns_drop.loc[missing_sqft_mask, 'zip'].map(total_sqft_median_by_zip)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['total_square_feet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then again for above_grade_square_feet\n",
    "df_columns_drop['above_grade_square_feet'] = pd.to_numeric(df_columns_drop['above_grade_square_feet'], errors='coerce')\n",
    "\n",
    "above_grade_sqft_median_by_zip = df_columns_drop.groupby('zip')['above_grade_square_feet'].median()\n",
    "\n",
    "missing_above_sqft_mask = df_columns_drop['above_grade_square_feet'].isna() & df_columns_drop['zip'].notna()\n",
    "df_columns_drop.loc[missing_above_sqft_mask, 'above_grade_square_feet'] = df_columns_drop.loc[missing_above_sqft_mask, 'zip'].map(above_grade_sqft_median_by_zip)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['above_grade_square_feet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean garages by calculating the median of houses with similar total_square_feet\n",
    "df_columns_drop['garages'] = pd.to_numeric(df_columns_drop['garages'], errors='coerce')\n",
    "df_columns_drop['total_square_feet'] = pd.to_numeric(df_columns_drop['total_square_feet'], errors='coerce')\n",
    "\n",
    "size_tolerance = 200\n",
    "min_sqft = df_columns_drop['total_square_feet'].min()\n",
    "max_sqft = df_columns_drop['total_square_feet'].max()\n",
    "bins = range(int(min_sqft), int(max_sqft) + size_tolerance, size_tolerance)\n",
    "df_columns_drop['sqft_bin'] = pd.cut(df_columns_drop['total_square_feet'], bins)\n",
    "\n",
    "median_garages_by_bin = df_columns_drop.groupby('sqft_bin')['garages'].median()\n",
    "\n",
    "missing_garages_mask = df_columns_drop['garages'].isna() & df_columns_drop['sqft_bin'].notna()\n",
    "df_columns_drop.loc[missing_garages_mask, 'garages'] = df_columns_drop.loc[missing_garages_mask, 'sqft_bin'].map(median_garages_by_bin)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['garages'])\n",
    "\n",
    "df_columns_drop = df_columns_drop.drop(columns=['sqft_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean hoa_fee from the same zip\n",
    "df_columns_drop['hoa_fee'] = pd.to_numeric(df_columns_drop['hoa_fee'], errors='coerce')\n",
    "\n",
    "hoa_fee_median_by_zip = df_columns_drop.groupby('zip')['hoa_fee'].median()\n",
    "\n",
    "missing_hoa_mask = df_columns_drop['hoa_fee'].isna() & df_columns_drop['zip'].notna()\n",
    "df_columns_drop.loc[missing_hoa_mask, 'hoa_fee'] = df_columns_drop.loc[missing_hoa_mask, 'zip'].map(hoa_fee_median_by_zip)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['hoa_fee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sold_on by add the average days to listed_on\n",
    "df_columns_drop['listed_on'] = pd.to_datetime(df_columns_drop['listed_on'], errors='coerce')\n",
    "df_columns_drop['sold_on'] = pd.to_datetime(df_columns_drop['sold_on'], errors='coerce')\n",
    "\n",
    "missing_sold_mask = df_columns_drop['sold_on'].isna() & df_columns_drop['listed_on'].notna()\n",
    "df_columns_drop.loc[missing_sold_mask, 'sold_on'] = (\n",
    "    df_columns_drop.loc[missing_sold_mask, 'listed_on'] + pd.to_timedelta(average_days_on_market, unit='d')\n",
    ")\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['sold_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill sold_price by the list_price, then from the same zip\n",
    "df_columns_drop['sold_price'] = pd.to_numeric(df_columns_drop['sold_price'], errors='coerce')\n",
    "df_columns_drop['list_price'] = pd.to_numeric(df_columns_drop['list_price'], errors='coerce')\n",
    "\n",
    "missing_sold_price_mask = df_columns_drop['sold_price'].isna() & df_columns_drop['list_price'].notna()\n",
    "df_columns_drop.loc[missing_sold_price_mask, 'sold_price'] = df_columns_drop.loc[missing_sold_price_mask, 'list_price']\n",
    "\n",
    "missing_sold_price_mask = df_columns_drop['sold_price'].isna() & df_columns_drop['zip'].notna()\n",
    "sold_price_median_by_zip = df_columns_drop.groupby('zip')['sold_price'].median()\n",
    "df_columns_drop.loc[missing_sold_price_mask, 'sold_price'] = df_columns_drop.loc[missing_sold_price_mask, 'zip'].map(sold_price_median_by_zip)\n",
    "\n",
    "df_columns_drop = df_columns_drop.dropna(subset=['sold_price'])\n",
    "\n",
    "df_columns_drop = df_columns_drop.drop(columns=['list_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is clean\n",
    "df_columns_drop.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "# Column Adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year to new columns\n",
    "df_columns_drop['sold_on_year'] = pd.to_datetime(df_columns_drop['sold_on']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating age column\n",
    "df_columns_drop[\"age_of_building\"] = df_columns_drop['sold_on_year'] - df_columns_drop['year_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop after done\n",
    "df_columns_drop = df_columns_drop.drop(columns=['sold_on_year'])\n",
    "df_columns_drop = df_columns_drop.drop(columns=['year_built'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of negative values\n",
    "negative = (df_columns_drop['age_of_building'] < 0).sum()\n",
    "print(f'Negative: {negative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the postive\n",
    "df_columns_drop = df_columns_drop[df_columns_drop['age_of_building'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns\n",
    "df_columns_drop['listed_on'] = pd.to_datetime(df_columns_drop['listed_on'])\n",
    "df_columns_drop['sold_on'] = pd.to_datetime(df_columns_drop['sold_on'])\n",
    "\n",
    "# Swap dates if listed_on > sold_on\n",
    "mask = df_columns_drop['listed_on'] > df_columns_drop['sold_on']\n",
    "df_columns_drop.loc[mask, ['listed_on', 'sold_on']] = df_columns_drop.loc[mask, ['sold_on', 'listed_on']].values\n",
    "\n",
    "# Create days_on_market column\n",
    "df_columns_drop['days_on_market'] = (df_columns_drop['sold_on'] - df_columns_drop['listed_on']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop after done\n",
    "df_columns_drop = df_columns_drop.drop(columns=['sold_on'])\n",
    "df_columns_drop = df_columns_drop.drop(columns=['listed_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = (df_columns_drop['age_of_building'] < 0).sum()\n",
    "print(f'Negative: {negative}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy before dummy\n",
    "df_visualization = df_columns_drop.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop = pd.get_dummies(df_columns_drop, columns =['property_type'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop = pd.get_dummies(df_columns_drop, columns =['structural_type'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_drop.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_machine_learning = df_columns_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR for sold_price\n",
    "Q1 = df_columns_drop['sold_price'].quantile(0.25)\n",
    "Q3 = df_columns_drop['sold_price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "df_columns_drop = df_columns_drop[\n",
    "    (df_columns_drop['sold_price'] >= lower_bound) &\n",
    "    (df_columns_drop['sold_price'] <= upper_bound)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'above_grade_square_feet',\n",
    "    'total_square_feet',\n",
    "    'garages',\n",
    "    'beds',\n",
    "    'baths',\n",
    "    'zip',\n",
    "    'hoa_fee',\n",
    "    'age_of_building',\n",
    "    'days_on_market',\n",
    "    'property_type_Detached Single',\n",
    "    'property_type_Mobile Home',\n",
    "    'property_type_Multi-family',\n",
    "    'property_type_Two to Four Units',\n",
    "    'structural_type_Condo',\n",
    "    'structural_type_Detached'\n",
    "]\n",
    "\n",
    "target = ['sold_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_machine_learning[features]\n",
    "y = df_machine_learning[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_machine_learning[features]\n",
    "y = df_machine_learning[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose either StandardScaler() or MinMaxScaler()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "rf_regressor = RandomForestRegressor (\n",
    "    # Number of trees\n",
    "    n_estimators = 100,\n",
    "    # Depth of each tree\n",
    "    max_depth = 50,\n",
    "    # Minimum required sample\n",
    "    min_samples_split = 15,\n",
    "    # Minimum to be a leaf\n",
    "    min_samples_leaf = 2,\n",
    "    # Considered for splitting\n",
    "    max_features = 0.8,\n",
    "    # Apply randomness\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rf_regressor.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "rf_predict = rf_regressor.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform calculations\n",
    "mse = mean_squared_error(y_test, rf_predict)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "rmse = mean_squared_error(y_test, rf_predict, squared=False)\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "r2 = r2_score(y_test, rf_predict)\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMatrix fpr train and test\n",
    "dtrain = xgb. DMatrix(X_train, label=y_train)\n",
    "dtest = xgb. DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameters\n",
    "params = {\n",
    "    # Maximum depth of each decision tree\n",
    "    'max_depth': 16,\n",
    "    # Learning rate\n",
    "    'eta': 0.08,\n",
    "    # Loss function\n",
    "    'objective': 'reg:squarederror',\n",
    "    # Number of parallel threads\n",
    "    'nthread': 4,\n",
    "    # Number of estimators\n",
    "    'n_estimators': 100,\n",
    "    # Fraction of training data to be radom\n",
    "    'subsample':0.9,\n",
    "    # Fraction of prediction data to be random\n",
    "    'colsample_bytree': 0.8,\n",
    "    # Minimum loss reduction\n",
    "    'gamma': 0,\n",
    "    # L1 regularization\n",
    "    'reg_alpha': 0,\n",
    "    # L2 regularization\n",
    "    'reg_lambda': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training rounds\n",
    "num_boost_round = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "xgb_predict = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, xgb_predict)\n",
    "print(f'MSE: {mse}')\n",
    "\n",
    "rmse = mean_squared_error(y_test, xgb_predict, squared=False)\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "r2 = r2_score(y_test, xgb_predict)\n",
    "print(f'R^2 Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_columns_drop[features]\n",
    "y = df_columns_drop[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "# Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\n",
    "    'above_grade_square_feet': float(input(\"Enter the area of your livable space (exclude basement): \")),\n",
    "    'total_square_feet': float(input(\"Enter your house total area: \")),\n",
    "    'garages': int(input(\"How many garages does your house have: \")),\n",
    "    'beds': int(input(\"How many beds does your house have: \")),\n",
    "    'baths': int(input(\"How many baths does your house have: \")),\n",
    "    'zip': int(input(\"What is the zip code of your house: \")),\n",
    "    'hoa_fee': float(input(\"Enter your HOA fee (0 if none): \")),\n",
    "    'age_of_building': float(input(\"Enter the age of your building: \")),\n",
    "    'days_on_market': int(input(\"Enter average days on market (0 if unknown): \")),\n",
    "    'property_type_Detached Single': input(\"Is your property type Detached Single? (yes/no): \"),\n",
    "    'property_type_Mobile Home': input(\"Is your property type Mobile Home? (yes/no): \"),\n",
    "    'property_type_Multi-family': input(\"Is your property type Multi-family? (yes/no): \"),\n",
    "    'property_type_Two to Four Units': input(\"Is your property type Two to Four Units? (yes/no): \"),\n",
    "    'structural_type_Condo': input(\"Is your building structural type Condo? (yes/no): \"),\n",
    "    'structural_type_Detached': input(\"Is your building structural type Detached? (yes/no): \"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame([user_input])\n",
    "\n",
    "mapping_dictionary = {'no': 0, 'yes': 1}\n",
    "\n",
    "property_columns = [\n",
    "    'property_type_Detached Single',\n",
    "    'property_type_Mobile Home',\n",
    "    'property_type_Multi-family',\n",
    "    'property_type_Two to Four Units'\n",
    "]\n",
    "\n",
    "structural_columns = [\n",
    "    'structural_type_Condo',\n",
    "    'structural_type_Detached'\n",
    "]\n",
    "\n",
    "for column in property_columns + structural_columns:\n",
    "    user_df[column] = user_df[column].map(mapping_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prediction = lgb_model.predict(user_df)\n",
    "\n",
    "print(f\"The predicted sold price for your house is: {user_prediction[0]:,.2f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "# Data Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset for clustering\n",
    "df_clustering = df_columns_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reselect the feature and target columns\n",
    "clustering_df = df_clustering[[\n",
    "    'above_grade_square_feet',\n",
    "    'total_square_feet',\n",
    "    'garages',\n",
    "    'beds',\n",
    "    'baths',\n",
    "    'zip',\n",
    "    'hoa_fee',\n",
    "    'age_of_building',\n",
    "    'days_on_market',\n",
    "    'property_type_Detached Single',\n",
    "    'property_type_Mobile Home',\n",
    "    'property_type_Multi-family',\n",
    "    'property_type_Two to Four Units',\n",
    "    'structural_type_Condo',\n",
    "    'structural_type_Detached'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal number of cluster that can be formed\n",
    "Elbow = KElbowVisualizer (KMeans(), k = 10)\n",
    "Elbow.fit(clustering_df)\n",
    "Elbow.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the houses to their relevant cluster\n",
    "num_clusters = \n",
    "kmeans = KMeans(n_clusters = num_clusters)\n",
    "kmeans.fit(clustering_df)\n",
    "clustering_df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 0\n",
    "clustering_0 = clustering_df[clustering_df['Cluster'] == 0]\n",
    "clustering_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 1\n",
    "clustering_1 = clustering_df[clustering_df['Cluster'] == 1]\n",
    "clustering_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 2\n",
    "clustering_2 = clustering_df[clustering_df['Cluster'] == 2]\n",
    "clustering_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 3\n",
    "clustering_3 = clustering_df[clustering_df['Cluster'] == 3]\n",
    "clustering_3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 4\n",
    "clustering_4 = clustering_df[clustering_df['Cluster'] == 4]\n",
    "clustering_4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 5\n",
    "clustering_5 = clustering_df[clustering_df['Cluster'] == 5]\n",
    "clustering_5.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "df_visualization = df_columns_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "numeric_df = df_columns_drop[features + target]\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "fig = px.imshow(\n",
    "    corr_matrix,\n",
    "    text_auto=True,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    width=1200,\n",
    "    height=1200\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "scatter_features = [\n",
    "    'above_grade_square_feet',\n",
    "    'total_square_feet',\n",
    "    'beds',\n",
    "    'baths',\n",
    "    'garages',\n",
    "    'hoa_fee',\n",
    "    'age_of_building',\n",
    "    'days_on_market'\n",
    "]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=4,\n",
    "    subplot_titles=scatter_features\n",
    ")\n",
    "\n",
    "row = 1\n",
    "col = 1\n",
    "\n",
    "for feature in scatter_features:\n",
    "    fig.add_trace(\n",
    "        px.scatter(\n",
    "            df_columns_drop,\n",
    "            x=feature,\n",
    "            y='sold_price'\n",
    "        ).data[0],\n",
    "        row=row,\n",
    "        col=col\n",
    "    )\n",
    "\n",
    "    col += 1\n",
    "    if col > 4:\n",
    "        col = 1\n",
    "        row += 1\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1600,\n",
    "    title_text=\"Key Features vs Sold Price\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots\n",
    "cat_features = [\n",
    "    'property_type_Detached Single',\n",
    "    'property_type_Mobile Home',\n",
    "    'property_type_Multi-family',\n",
    "    'property_type_Two to Four Units',\n",
    "    'structural_type_Condo',\n",
    "    'structural_type_Detached'\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3, subplot_titles=cat_features)\n",
    "\n",
    "row = 1\n",
    "col = 1\n",
    "for feature in cat_features:\n",
    "    fig.add_trace(\n",
    "        px.box(df_columns_drop, x=feature, y='sold_price').data[0],\n",
    "        row=row, col=col\n",
    "    )\n",
    "    col += 1\n",
    "    if col > 3:\n",
    "        col = 1\n",
    "        row += 1\n",
    "\n",
    "fig.update_layout(height=800, width=1600, title_text=\"Sold Price Distribution by Categorical Features\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "# Data Dictionary\n",
    "\n",
    "## Target\n",
    "| Column | Description | Type |\n",
    "|--------|-------------|------|\n",
    "| sold_price | Final selling price of the property in USD | float |\n",
    "\n",
    "## Numeric Features\n",
    "| Column | Description | Type |\n",
    "|--------|-------------|------|\n",
    "| above_grade_square_feet | Square footage of the livable area excluding basement | float |\n",
    "| total_square_feet | Total square footage of the property | float |\n",
    "| garages | Number of garages in the property | float |\n",
    "| beds | Number of bedrooms | float |\n",
    "| baths | Number of bathrooms | float |\n",
    "| zip | ZIP code of the property location | float |\n",
    "| hoa_fee | Monthly Homeowner Association fee in USD | float |\n",
    "| age_of_building | Age of the building in years | float |\n",
    "| days_on_market | Number of days the property was listed before being sold | int |\n",
    "\n",
    "## Categorical / Boolean Features\n",
    "| Column | Description | Type |\n",
    "|--------|-------------|------|\n",
    "| property_type_Detached Single | Indicates if property type is Detached Single | bool |\n",
    "| property_type_Mobile Home | Indicates if property type is Mobile Home | bool |\n",
    "| property_type_Multi-family | Indicates if property type is Multi-family | bool |\n",
    "| property_type_Two to Four Units | Indicates if property type is Two to Four Units | bool |\n",
    "| structural_type_Condo | Indicates if the building is a Condo | bool |\n",
    "| structural_type_Detached | Indicates if the building is Detached | bool |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
